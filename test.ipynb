{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data has been saved to Lunyu_Corpus.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'Lunyu Complete Analysis.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize an empty list to store the corpus data\n",
    "corpus_data = []\n",
    "\n",
    "# Define the number of rows in a set\n",
    "rows_per_set = 6\n",
    "\n",
    "# Loop through the CSV file in steps of rows_per_set to process each set\n",
    "for start_index in range(0, len(df), rows_per_set):\n",
    "    # Check if the set is complete\n",
    "    if start_index + rows_per_set > len(df):\n",
    "        break\n",
    "    \n",
    "    # Extract the data for one set\n",
    "    id_value = str(df.iloc[start_index, 0])\n",
    "    meaning_value = str(df.iloc[start_index + 4, 0]) if pd.notna(df.iloc[start_index + 4, 0]) else \"\"\n",
    "    \n",
    "    # Extract the text details\n",
    "    text_data = []\n",
    "    for col_index in range(1, df.shape[1]):\n",
    "        if pd.notna(df.iloc[start_index + 1, col_index]):\n",
    "            text_data.append({\n",
    "                \"unicode\": str(df.iloc[start_index, col_index]),\n",
    "                \"character\": str(df.iloc[start_index + 1, col_index]),\n",
    "                \"pronunciation\": str(df.iloc[start_index + 2, col_index]),\n",
    "                \"chinese\": str(df.iloc[start_index + 3, col_index])\n",
    "            })\n",
    "\n",
    "    # Append the extracted set to the corpus data list\n",
    "    corpus_data.append({\n",
    "        \"id\": id_value,\n",
    "        \"metadata\": {\n",
    "            \"meaning\": meaning_value\n",
    "        },\n",
    "        \"text\": text_data\n",
    "    })\n",
    "\n",
    "# Convert the corpus data to JSON format\n",
    "json_output = {\n",
    "    \"corpus_data\": corpus_data\n",
    "}\n",
    "\n",
    "# Save the JSON data to a file\n",
    "output_path = 'Lunyu_Corpus.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(json_output, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"JSON data has been saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'Lunyu Complete Analysis.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Define the number of rows in a set\n",
    "rows_per_set = 6\n",
    "\n",
    "# Process each set\n",
    "for start_index in range(0, len(df), rows_per_set):\n",
    "    # Check if the set is complete\n",
    "    if start_index + rows_per_set > len(df):\n",
    "        break\n",
    "    \n",
    "    # Combine the rows by concatenating them horizontally\n",
    "    combined_row = pd.concat([df.iloc[start_index + i] for i in range(rows_per_set)], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Append the combined row to the DataFrame\n",
    "    combined_df = pd.concat([combined_df, combined_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV data has been saved to Combined_Lunyu.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the combined DataFrame to a new CSV file\n",
    "output_combined_csv_path = 'Combined_Lunyu.csv'\n",
    "combined_df.to_csv(output_combined_csv_path, index=False)\n",
    "\n",
    "print(f\"Combined CSV data has been saved to {output_combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0       1     2    3    4     5     6       7      8    \\\n",
      "0               0316b1¬†     NaN   NaN  NaN  NaN   NaN   NaN     NaN    NaN   \n",
      "1                   NaN     NaN   NaN  NaN  NaN   NaN   NaN     NaN    NaN   \n",
      "2  ‰∏çÁÇ∫ÈÅéÁü£„ÄÇËìãÊúâÊñáÂ≠ê‰πãË≥™ÔºåÂÜçÊñØÂèØÁü£„ÄÇÁÑ°ÊñáÂ≠ê     NaN   NaN  NaN  NaN   NaN   NaN     NaN    NaN   \n",
      "3                     Â¶Ç       Ë≥™    ÁÑ°„ÄÇ    ‰∏â    ÊÄù     Èùû    ÂâáÔºõ       ‰∏ç     ÂèØ„ÄÇ   \n",
      "4                  lj…®1  lhjw…®2  ≈ãwu2  k·ª•2  ku1  mji1  ≈∫we1  nioow1  ≈∫ier1   \n",
      "\n",
      "    9    ...   98     99    100   101     102  103   104    105  106  107  \n",
      "0   NaN  ...   NaN    NaN   NaN   NaN     NaN  NaN   NaN    NaN  NaN  NaN  \n",
      "1   NaN  ...     Ë≥™     Êúâ„ÄÇ     ÂÜç    ÂâáÔºõ       ÂèØ   ‰πü„ÄÇ   [Êñá]    [Â≠ê]  NaN  NaN  \n",
      "2   NaN  ...  lew1  goor1   no2   s·ªç1  sjiij2  do2   ku1  lj…®…®1  NaN  NaN  \n",
      "3   Â≠î„ÄÄÂ≠ê  ...     óñ£      òÉû     òçû     òìê       óñµ    òòù     òÉ°      ó°∂  NaN  NaN  \n",
      "4  lj…®1  ...  4925   4612  3433  3849    4859  197  4310   1718  NaN  NaN  \n",
      "\n",
      "[5 rows x 108 columns]\n",
      "Combined CSV data has been saved to 2Combined_Lunyu.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'Lunyu Complete Analysis.csv'\n",
    "df = pd.read_csv(file_path, header=None)  # No header, as each row is part of a set\n",
    "\n",
    "# Initialize an empty list to store combined rows\n",
    "combined_data = []\n",
    "\n",
    "# Define the number of rows in a set\n",
    "rows_per_set = 6\n",
    "\n",
    "# Process each set\n",
    "for start_index in range(0, len(df), rows_per_set):\n",
    "    # Check if the set is complete\n",
    "    if start_index + rows_per_set > len(df):\n",
    "        break\n",
    "    \n",
    "    # Extract and combine the 6 rows into a single row\n",
    "    combined_row = []\n",
    "    for i in range(rows_per_set):\n",
    "        combined_row.extend(df.iloc[start_index + i].tolist())\n",
    "    \n",
    "    # Append the combined row to the list\n",
    "    combined_data.append(combined_row)\n",
    "\n",
    "# Create a DataFrame from the combined data\n",
    "combined_df = pd.DataFrame(combined_data)\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(combined_df.head())\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "output_combined_csv_path = '2Combined_Lunyu.csv'\n",
    "combined_df.to_csv(output_combined_csv_path, index=False, header=False)\n",
    "\n",
    "print(f\"Combined CSV data has been saved to {output_combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0316b1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Ë≥™</td>\n",
       "      <td>Êúâ„ÄÇ</td>\n",
       "      <td>ÂÜç</td>\n",
       "      <td>ÂâáÔºõ</td>\n",
       "      <td>ÂèØ</td>\n",
       "      <td>‰πü„ÄÇ</td>\n",
       "      <td>[Êñá]</td>\n",
       "      <td>[Â≠ê]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‰∏çÁÇ∫ÈÅéÁü£„ÄÇËìãÊúâÊñáÂ≠ê‰πãË≥™ÔºåÂÜçÊñØÂèØÁü£„ÄÇÁÑ°ÊñáÂ≠ê</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>lew1</td>\n",
       "      <td>goor1</td>\n",
       "      <td>no2</td>\n",
       "      <td>s·ªç1</td>\n",
       "      <td>sjiij2</td>\n",
       "      <td>do2</td>\n",
       "      <td>ku1</td>\n",
       "      <td>lj…®…®1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Â¶Ç</td>\n",
       "      <td>Ë≥™</td>\n",
       "      <td>ÁÑ°„ÄÇ</td>\n",
       "      <td>‰∏â</td>\n",
       "      <td>ÊÄù</td>\n",
       "      <td>Èùû</td>\n",
       "      <td>ÂâáÔºõ</td>\n",
       "      <td>‰∏ç</td>\n",
       "      <td>ÂèØ„ÄÇ</td>\n",
       "      <td>Â≠î„ÄÄÂ≠ê</td>\n",
       "      <td>...</td>\n",
       "      <td>óñ£</td>\n",
       "      <td>òÉû</td>\n",
       "      <td>òçû</td>\n",
       "      <td>òìê</td>\n",
       "      <td>óñµ</td>\n",
       "      <td>òòù</td>\n",
       "      <td>òÉ°</td>\n",
       "      <td>ó°∂</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lj…®1</td>\n",
       "      <td>lhjw…®2</td>\n",
       "      <td>≈ãwu2</td>\n",
       "      <td>k·ª•2</td>\n",
       "      <td>ku1</td>\n",
       "      <td>mji1</td>\n",
       "      <td>≈∫we1</td>\n",
       "      <td>nioow1</td>\n",
       "      <td>≈∫ier1</td>\n",
       "      <td>lj…®1</td>\n",
       "      <td>...</td>\n",
       "      <td>4925</td>\n",
       "      <td>4612</td>\n",
       "      <td>3433</td>\n",
       "      <td>3849</td>\n",
       "      <td>4859</td>\n",
       "      <td>197</td>\n",
       "      <td>4310</td>\n",
       "      <td>1718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>NaN</td>\n",
       "      <td>‰∫∫</td>\n",
       "      <td>Âêõ</td>\n",
       "      <td>Â§©</td>\n",
       "      <td>Êñº</td>\n",
       "      <td>ÂâáÔºõ</td>\n",
       "      <td>Áå∂</td>\n",
       "      <td>{Â≠ê</td>\n",
       "      <td>Áà∂</td>\n",
       "      <td>‰πãÊñº}</td>\n",
       "      <td>...</td>\n",
       "      <td>óñµ</td>\n",
       "      <td>òé™</td>\n",
       "      <td>óå≠</td>\n",
       "      <td>òì∫</td>\n",
       "      <td>óÇ∏</td>\n",
       "      <td>òò•</td>\n",
       "      <td>òïã</td>\n",
       "      <td>òÇ¨</td>\n",
       "      <td>óÜ™</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>NaN</td>\n",
       "      <td>bju1</td>\n",
       "      <td>tshjiij1</td>\n",
       "      <td>ku1</td>\n",
       "      <td>m…ô1</td>\n",
       "      <td>zj…®Ã£1</td>\n",
       "      <td>¬∑j…®2</td>\n",
       "      <td>wo2</td>\n",
       "      <td>bju1</td>\n",
       "      <td>tshjiij1</td>\n",
       "      <td>...</td>\n",
       "      <td>460</td>\n",
       "      <td>1553</td>\n",
       "      <td>1491</td>\n",
       "      <td>3125</td>\n",
       "      <td>3612</td>\n",
       "      <td>1824</td>\n",
       "      <td>972</td>\n",
       "      <td>4853</td>\n",
       "      <td>2778</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>NaN</td>\n",
       "      <td>òñë</td>\n",
       "      <td>òúï</td>\n",
       "      <td>óÖã</td>\n",
       "      <td>óπë</td>\n",
       "      <td>òñë</td>\n",
       "      <td>òúï</td>\n",
       "      <td>òüÇ</td>\n",
       "      <td>òì∫</td>\n",
       "      <td>óÇ∏</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3510</td>\n",
       "      <td>4979</td>\n",
       "      <td>3687</td>\n",
       "      <td>4391</td>\n",
       "      <td>1824</td>\n",
       "      <td>2386</td>\n",
       "      <td>4329</td>\n",
       "      <td>5615</td>\n",
       "      <td>1348</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>1017a8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>435 rows √ó 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0       1         2     3     4      5     6       7    \\\n",
       "0                 0316b1¬†     NaN       NaN   NaN   NaN    NaN   NaN     NaN   \n",
       "1                     NaN     NaN       NaN   NaN   NaN    NaN   NaN     NaN   \n",
       "2    ‰∏çÁÇ∫ÈÅéÁü£„ÄÇËìãÊúâÊñáÂ≠ê‰πãË≥™ÔºåÂÜçÊñØÂèØÁü£„ÄÇÁÑ°ÊñáÂ≠ê     NaN       NaN   NaN   NaN    NaN   NaN     NaN   \n",
       "3                       Â¶Ç       Ë≥™        ÁÑ°„ÄÇ     ‰∏â     ÊÄù      Èùû    ÂâáÔºõ       ‰∏ç   \n",
       "4                    lj…®1  lhjw…®2      ≈ãwu2   k·ª•2   ku1   mji1  ≈∫we1  nioow1   \n",
       "..                    ...     ...       ...   ...   ...    ...   ...     ...   \n",
       "430                   NaN       ‰∫∫         Âêõ     Â§©     Êñº     ÂâáÔºõ     Áå∂      {Â≠ê   \n",
       "431                   NaN    bju1  tshjiij1   ku1   m…ô1  zj…®Ã£1  ¬∑j…®2     wo2   \n",
       "432                   NaN       òñë         òúï     óÖã     óπë      òñë     òúï       òüÇ   \n",
       "433                   NaN    3510      4979  3687  4391   1824  2386    4329   \n",
       "434                1017a8     NaN       NaN   NaN   NaN    NaN   NaN     NaN   \n",
       "\n",
       "       8         9    ...   98     99    100   101     102   103   104    105  \\\n",
       "0      NaN       NaN  ...   NaN    NaN   NaN   NaN     NaN   NaN   NaN    NaN   \n",
       "1      NaN       NaN  ...     Ë≥™     Êúâ„ÄÇ     ÂÜç    ÂâáÔºõ       ÂèØ    ‰πü„ÄÇ   [Êñá]    [Â≠ê]   \n",
       "2      NaN       NaN  ...  lew1  goor1   no2   s·ªç1  sjiij2   do2   ku1  lj…®…®1   \n",
       "3       ÂèØ„ÄÇ       Â≠î„ÄÄÂ≠ê  ...     óñ£      òÉû     òçû     òìê       óñµ     òòù     òÉ°      ó°∂   \n",
       "4    ≈∫ier1      lj…®1  ...  4925   4612  3433  3849    4859   197  4310   1718   \n",
       "..     ...       ...  ...   ...    ...   ...   ...     ...   ...   ...    ...   \n",
       "430      Áà∂       ‰πãÊñº}  ...     óñµ      òé™     óå≠     òì∫       óÇ∏     òò•     òïã      òÇ¨   \n",
       "431   bju1  tshjiij1  ...   460   1553  1491  3125    3612  1824   972   4853   \n",
       "432      òì∫         óÇ∏  ...   NaN    NaN   NaN   NaN     NaN   NaN   NaN    NaN   \n",
       "433   5615      1348  ...   NaN    NaN   NaN   NaN     NaN   NaN   NaN    NaN   \n",
       "434    NaN       NaN  ...   NaN    NaN   NaN   NaN     NaN   NaN   NaN    NaN   \n",
       "\n",
       "      106  107  \n",
       "0     NaN  NaN  \n",
       "1     NaN  NaN  \n",
       "2     NaN  NaN  \n",
       "3     NaN  NaN  \n",
       "4     NaN  NaN  \n",
       "..    ...  ...  \n",
       "430     óÜ™  NaN  \n",
       "431  2778  NaN  \n",
       "432   NaN  NaN  \n",
       "433   NaN  NaN  \n",
       "434   NaN  NaN  \n",
       "\n",
       "[435 rows x 108 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed CSV saved to output2.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def transform_csv(input_file, output_file):\n",
    "    with open(input_file, newline='', encoding='utf-8') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        # ÂàùÊúüÂåñ\n",
    "        combined_rows = [[] for _ in range(6)]\n",
    "        row_index = 0\n",
    "        \n",
    "        for row in reader:\n",
    "            if not any(row):  # Á©∫Ë°å„ÅÆÊ§úÂá∫\n",
    "                row_index = 0\n",
    "                continue\n",
    "            \n",
    "            combined_rows[row_index].extend(row)\n",
    "            row_index += 1\n",
    "            \n",
    "            # ÂêÑ„Çª„ÉÉ„Éà„ÅØ6Ë°å„Åß1„Çª„ÉÉ„Éà„Å™„ÅÆ„Åß„ÄÅ6Ë°åÁõÆ„ÅÆÂæå„ÅØÊ¨°„ÅÆ„Çª„ÉÉ„Éà„Å´Áßª„Çã\n",
    "            if row_index == 6:\n",
    "                row_index = 0\n",
    "\n",
    "        # Â§âÊèõ„Åó„ÅüË°å„ÇíÊõ∏„ÅçÂá∫„Åô\n",
    "        for row in combined_rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# ‰ΩøÁî®‰æã\n",
    "input_file = 'Lunyu Complete Analysis.csv'\n",
    "output_file = 'output2.csv'\n",
    "transform_csv(input_file, output_file)\n",
    "\n",
    "print(f\"Transformed CSV saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed JSON saved to output.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_json(input_file, output_file):\n",
    "    with open(input_file, newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        rows = list(reader)\n",
    "    \n",
    "    # JSON„Éá„Éº„Çø„Çí‰øùÊåÅ„Åô„Çã„É™„Çπ„Éà\n",
    "    corpus_data = []\n",
    "    \n",
    "    # ÂêÑ„Çª„ÉÉ„ÉàÔºà6Ë°å„Åö„Å§Ôºâ„ÇíÂá¶ÁêÜ\n",
    "    num_sets = len(rows[0]) // 4  # ÂêÑ„Çª„ÉÉ„Éà„Åå4ÂàóÔºàid, meaning, text, pronunciationÔºâ„ÅßÊßãÊàê„Åï„Çå„Çã„Å®‰ªÆÂÆö\n",
    "    \n",
    "    for i in range(num_sets):\n",
    "        # idÂàó„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ\n",
    "        idx = i * 4\n",
    "        \n",
    "        corpus_entry = {\n",
    "            \"id\": rows[0][idx],\n",
    "            \"metadata\": {\n",
    "                \"meaning\": rows[5][idx]  # ÊúÄÂæå„ÅÆË°å„Ååmeaning\n",
    "            },\n",
    "            \"text\": []\n",
    "        }\n",
    "        \n",
    "        for j in range(4):\n",
    "            text_entry = {\n",
    "                \"unicode\": rows[1][idx + j],\n",
    "                \"character\": rows[2][idx + j],\n",
    "                \"pronunciation\": rows[3][idx + j],\n",
    "                \"chinese\": rows[4][idx + j]\n",
    "            }\n",
    "            corpus_entry[\"text\"].append(text_entry)\n",
    "        \n",
    "        corpus_data.append(corpus_entry)\n",
    "    \n",
    "    # JSON„Éï„Ç°„Ç§„É´„Å®„Åó„Å¶‰øùÂ≠ò\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump({\"corpus_data\": corpus_data}, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "# ‰ΩøÁî®‰æã\n",
    "input_file = 'test_row_data.csv'\n",
    "output_file = 'output.json'\n",
    "csv_to_json(input_file, output_file)\n",
    "\n",
    "print(f\"Transformed JSON saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed JSON saved to output3.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_json(input_file, output_file):\n",
    "    with open(input_file, newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        rows = list(reader)\n",
    "    \n",
    "    # JSON„Éá„Éº„Çø„Çí‰øùÊåÅ„Åô„Çã„É™„Çπ„Éà\n",
    "    corpus_data = []\n",
    "    \n",
    "    # ÁèæÂú®Âá¶ÁêÜ‰∏≠„ÅÆ„Çª„ÉÉ„Éà„Çí‰øùÊåÅ„Åô„ÇãÂ§âÊï∞\n",
    "    current_set = {}\n",
    "    text_entries = []\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        if not any(row):  # Á©∫Ë°å„ÅÆÊ§úÂá∫\n",
    "            if current_set:  # Êó¢Â≠ò„ÅÆ„Çª„ÉÉ„Éà„Åå„ÅÇ„ÇãÂ†¥Âêà\n",
    "                current_set[\"text\"] = text_entries\n",
    "                corpus_data.append(current_set)\n",
    "                current_set = {}\n",
    "                text_entries = []\n",
    "            continue\n",
    "        \n",
    "        # Ë°å„ÅÆÁï™Âè∑„Å´Âü∫„Å•„ÅÑ„Å¶„Çª„ÉÉ„Éà„Çí‰ΩúÊàê\n",
    "        if i % 7 == 0:\n",
    "            current_set[\"id\"] = row[0]\n",
    "        elif i % 7 == 1:\n",
    "            unicode_list = row\n",
    "        elif i % 7 == 2:\n",
    "            character_list = row\n",
    "        elif i % 7 == 3:\n",
    "            pronunciation_list = row\n",
    "        elif i % 7 == 4:\n",
    "            chinese_list = row\n",
    "        elif i % 7 == 5:\n",
    "            current_set[\"metadata\"] = {\"meaning\": row[0]}\n",
    "            # ÂêÑ„ÉÜ„Ç≠„Çπ„Éà„Ç®„É≥„Éà„É™„Éº„Çí‰ΩúÊàê\n",
    "            for u, c, p, ch in zip(unicode_list, character_list, pronunciation_list, chinese_list):\n",
    "                text_entries.append({\n",
    "                    \"unicode\": u,\n",
    "                    \"character\": c,\n",
    "                    \"pronunciation\": p,\n",
    "                    \"chinese\": ch\n",
    "                })\n",
    "    \n",
    "    # ÊúÄÂæå„ÅÆ„Çª„ÉÉ„Éà„ÇíËøΩÂä†\n",
    "    if current_set:\n",
    "        current_set[\"text\"] = text_entries\n",
    "        corpus_data.append(current_set)\n",
    "    \n",
    "    # JSON„Éï„Ç°„Ç§„É´„Å®„Åó„Å¶‰øùÂ≠ò\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump({\"corpus_data\": corpus_data}, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "# ‰ΩøÁî®‰æã\n",
    "input_file = 'Lunyu Complete Analysis.csv'\n",
    "output_file = 'output3.json'\n",
    "csv_to_json(input_file, output_file)\n",
    "\n",
    "print(f\"Transformed JSON saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed JSON saved to output4.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_json(input_file, output_file):\n",
    "    with open(input_file, newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        rows = list(reader)\n",
    "    \n",
    "    corpus_data = []  # JSON„Éá„Éº„Çø„Çí‰øùÊåÅ„Åô„Çã„É™„Çπ„Éà\n",
    "    \n",
    "    current_set = {}  # ÁèæÂú®Âá¶ÁêÜ‰∏≠„ÅÆ„Çª„ÉÉ„Éà„Çí‰øùÊåÅ„Åô„ÇãÂ§âÊï∞\n",
    "    text_entries = []  # „ÉÜ„Ç≠„Çπ„Éà„Ç®„É≥„Éà„É™„Çí‰øùÊåÅ„Åô„Çã„É™„Çπ„Éà\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        if not any(row):  # Á©∫Ë°å„ÅÆÊ§úÂá∫\n",
    "            if current_set:  # Êó¢Â≠ò„ÅÆ„Çª„ÉÉ„Éà„Åå„ÅÇ„ÇãÂ†¥Âêà\n",
    "                # Á©∫„ÅÆ„Ç®„É≥„Éà„É™„ÇíÈô§Â§ñ\n",
    "                current_set[\"text\"] = [entry for entry in text_entries if any(entry.values())]\n",
    "                corpus_data.append(current_set)\n",
    "                current_set = {}\n",
    "                text_entries = []\n",
    "            continue\n",
    "        \n",
    "        if i % 7 == 0:\n",
    "            current_set[\"id\"] = row[0]\n",
    "        elif i % 7 == 1:\n",
    "            unicode_list = row\n",
    "        elif i % 7 == 2:\n",
    "            character_list = row\n",
    "        elif i % 7 == 3:\n",
    "            pronunciation_list = row\n",
    "        elif i % 7 == 4:\n",
    "            chinese_list = row\n",
    "        elif i % 7 == 5:\n",
    "            current_set[\"metadata\"] = {\"meaning\": row[0]}\n",
    "            for u, c, p, ch in zip(unicode_list, character_list, pronunciation_list, chinese_list):\n",
    "                entry = {\n",
    "                    \"unicode\": u,\n",
    "                    \"character\": c,\n",
    "                    \"pronunciation\": p,\n",
    "                    \"chinese\": ch\n",
    "                }\n",
    "                text_entries.append(entry)\n",
    "    \n",
    "    if current_set:  # ÊúÄÂæå„ÅÆ„Çª„ÉÉ„Éà„ÇíËøΩÂä†\n",
    "        current_set[\"text\"] = [entry for entry in text_entries if any(entry.values())]\n",
    "        corpus_data.append(current_set)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump({\"corpus_data\": corpus_data}, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "# ‰ΩøÁî®‰æã\n",
    "input_file = 'Lunyu Complete Analysis.csv'\n",
    "output_file = 'output4.json'\n",
    "csv_to_json(input_file, output_file)\n",
    "\n",
    "print(f\"Transformed JSON saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
